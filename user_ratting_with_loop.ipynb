{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b916e34-85eb-4142-a3d5-45ca735e57f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing User ID: 14008088\n",
      "Scraping page 1\n",
      "Scraping URL: https://www.goodreads.com/review/list/14008088?order=d&page=1&sort=rating&view=reviews\n",
      "14116\n",
      "rating\n",
      "5\n",
      "14118\n",
      "rating\n",
      "5\n",
      "24812\n",
      "rating\n",
      "5\n",
      "72915\n",
      "rating\n",
      "5\n",
      "72910\n",
      "rating\n",
      "5\n",
      "76234\n",
      "rating\n",
      "5\n",
      "77174\n",
      "rating\n",
      "5\n",
      "7356990\n",
      "rating\n",
      "5\n",
      "87420\n",
      "rating\n",
      "5\n",
      "140130\n",
      "rating\n",
      "5\n",
      "146122\n",
      "rating\n",
      "5\n",
      "146128\n",
      "rating\n",
      "5\n",
      "166353\n",
      "rating\n",
      "5\n",
      "166356\n",
      "rating\n",
      "5\n",
      "166396\n",
      "rating\n",
      "5\n",
      "166399\n",
      "rating\n",
      "5\n",
      "179173\n",
      "rating\n",
      "5\n",
      "9602495\n",
      "rating\n",
      "5\n",
      "22446777\n",
      "rating\n",
      "5\n",
      "13498737\n",
      "rating\n",
      "5\n",
      "Scraping page 2\n",
      "Scraping URL: https://www.goodreads.com/review/list/14008088?order=d&page=2&sort=rating&view=reviews\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file containing the User IDs and URLs\n",
    "df_csv = pd.read_csv('User_Review_url_and_ID.csv')\n",
    "\n",
    "# Headers to simulate a browser\n",
    "headers = {\n",
    "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,/;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "    \"accept-language\": \"en-US,en;q=0.9\",\n",
    "    \"cache-control\": \"max-age=0\",\n",
    "    \"cookie\": 'ccsid=997-0059822-9974992; locale=en; blocking_sign_in_interstitial=true; csm-sid=448-9044706-8238887; session-id=138-9449735-4650719; lc-main=en_US; ubid-main=132-0831767-7645715; csm-hit=tb:s-HH6TEBC541XWDPX2HW08|1728400665806&t:1728400665807; session-id-time=2359120669l; session-token=\"9epd7M6HsVRPHteGVvJu5Fk8d+KvASerta8tz+la5w+TVREtncxVnfcCzs8K9EpSg+fQrgx1GgnsD2rLlayR4DmjBe1MsI/hzqbpLxW9fzltLac/Bx4GP/iysuf1f336QEHPYkFM5wvwKAJRlJF/gUtvoqUNgeHO7d0nUgwzQ4n10cVaWcJP8kU3S8U/EG5f1z4kuWhJEC/GxOGAFPM2AwR1aAfDTGc3oUmUkI4heyVFL2PVr5DhINiO23tre2UqOAtOSgArJ6foDhvdkRwB1A2cwygZeAAPu6znpJ+cxdwCK+Qw0zKdF5L4dt5MPkikWWxVqpvSWD4krg+mGhL2EJhFdAQbaWn2SM7qiWrwNN8=\"; x-main=\"qY09NsWCzSNW8b933Yonj4af6qs5TDB0DUsYapgS18@nyKFkHdJj5p@Mt2dldE6F\"; at-main=Atza|IwEBIKKe6dZi9mkGtv2CtS3qsv9NuaJNSNpmNpDSxRXvt17a_x2-zjEpiAOHkEQ34JuOVngoGfAl8dIqliYqOigT30w9CGWKtZI6rgDWz0mruXs-yNgr_nsxBWgaXid-1e8ZvmgQryKo5OgAIx-Lce0ocNWWSfh3g77KeZNETFdonWPAuJz5V82UuuKblmr-o03ZzeyBC7rbWUbFLRhtcSWy7Mqogs8ONE5dM3XgdBITj5b5F2h4Z_ih2WgCM4wTps3r6HY; sess-at-main=\"l6swvzfG2thASaG98K8Nu/j0D/+teOfY5a1UhpsZBz8=\"; likely_has_account=true; *session*id2=4cd4cabfc8c16e891c0f62348aa076dd',\n",
    "    \"if-none-match\": 'W/\"862d60fdc6915eed33d69a373690f36d\"',\n",
    "    \"priority\": \"u=0, i\",\n",
    "    \"sec-ch-ua\": '\"Google Chrome\";v=\"129\", \"Not=A?Brand\";v=\"8\", \"Chromium\";v=\"129\"',\n",
    "    \"sec-ch-ua-mobile\": \"?0\",\n",
    "    \"sec-ch-ua-platform\": '\"Windows\"',\n",
    "    \"sec-fetch-dest\": \"document\",\n",
    "    \"sec-fetch-mode\": \"navigate\",\n",
    "    \"sec-fetch-site\": \"none\",\n",
    "    \"sec-fetch-user\": \"?1\",\n",
    "    \"upgrade-insecure-requests\": \"1\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Initialize a DataFrame to store ratings\n",
    "ratings_data = []\n",
    "\n",
    "# Iterate through each User_ID in the CSV file\n",
    "for index, row in df_csv.iterrows():\n",
    "    user_id = row['User_ID']\n",
    "    print(f\"Processing User ID: {user_id}\")\n",
    "    \n",
    "    for i in range(1, 4):  # Loop through pages if needed, modify the range accordingly\n",
    "        print(f\"Scraping page {i}\")\n",
    "        url = f\"https://www.goodreads.com/review/list/{user_id}?order=d&page={i}&sort=rating&view=reviews\"\n",
    "        print(f\"Scraping URL: {url}\")\n",
    "        \n",
    "        # Request the page\n",
    "        response = requests.get(url, headers=headers)\n",
    "       # print(response)\n",
    "        # Parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all rows in the table where book ID and rating are stored\n",
    "        #rows = soup.find_all('tr', class_='bookalike review')\n",
    "        rows = soup.find_all('tr', class_='bookalike review')\n",
    "        #print(rows)\n",
    "\n",
    "        for row in rows:\n",
    "            # Step 1: Find the <td> with the book ID in the link\n",
    "            book_td = row.find('td', class_='field title')\n",
    "            #print(book_td)\n",
    "            if book_td:\n",
    "                bb = book_td.find('div', class_='value')\n",
    "                # print(bb)\n",
    "                a_tag = bb.find('a', href=True)\n",
    "               # print(a_tag)\n",
    "                if a_tag and '/book/show/' in a_tag['href']:\n",
    "                    book_url = a_tag['href']\n",
    "                    book_id = book_url.split('/book/show/')[1].split('-')[0].split('.')[0]\n",
    "                    print(book_id)\n",
    "                    # Step 2: Find the <td> with the rating (sibling or close td)\n",
    "                    rating_td = row.find('td', class_='field rating')\n",
    "                    if rating_td:\n",
    "                        rating_span = rating_td.find('span', class_='staticStars')\n",
    "                        if rating_span:\n",
    "                           # rating = rating_span['title']  # Extract the rating from 'title'\n",
    "                            stars = rating_td.find_all('span', class_='staticStar p10')\n",
    "                            print('rating')\n",
    "                            print(len(stars))# Append the data to the list\n",
    "                            rating=len(stars)\n",
    "                            ratings_data.append({\n",
    "                                'User_ID': user_id,\n",
    "                                'Book_ID': book_id,\n",
    "                                'Rating': rating\n",
    "                            })\n",
    "\n",
    "# Convert to DataFrame\n",
    "ratings_df = pd.DataFrame(ratings_data)\n",
    "\n",
    "# Display or save the DataFrame\n",
    "# print(ratings_df)\n",
    "\n",
    "# Optionally, save to a CSV file\n",
    "ratings_df.to_csv('scraped_ratings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80218a6c-6040-4d3f-8ce1-7858886a6515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
