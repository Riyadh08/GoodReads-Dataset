{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c5c14b3-7370-42ff-9ea1-5ade8e2f7d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique books fetched: 1773\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Set headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to fetch and parse a page\n",
    "def fetch_page(page_num):\n",
    "    url = f\"https://www.goodreads.com/review/list/16297867?page={page_num}\"\n",
    "\n",
    "    # Use a session to handle retries\n",
    "    session = requests.Session()\n",
    "    retry = Retry(connect=5, backoff_factor=0.5)  # Retry failed connections 5 times\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception for 4XX/5XX HTTP status codes\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching page {page_num}: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Parse the content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all divs with class 'value'\n",
    "    tempa = soup.find_all('div', class_='value')\n",
    "\n",
    "    # Extract book_id and URL\n",
    "    books = []\n",
    "    for div in tempa:\n",
    "        a_tag = div.find('a', href=True)\n",
    "        if a_tag:\n",
    "            book_url = a_tag['href']\n",
    "            if '/book/show/' in book_url:\n",
    "                # Extract book_id before '-' and '.'\n",
    "                book_id = book_url.split('/book/show/')[1].split('-')[0]\n",
    "                book_id = book_id.split('.')[0]\n",
    "                books.append({'book_id': book_id, 'url': f\"https://www.goodreads.com{book_url}\"})\n",
    "    \n",
    "    return books\n",
    "\n",
    "# Main function to fetch multiple pages concurrently\n",
    "def fetch_all_pages(total_pages=103):\n",
    "    all_books = []\n",
    "\n",
    "    # Use ThreadPoolExecutor to fetch multiple pages concurrently\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(fetch_page, i) for i in range(1, total_pages+1)]\n",
    "\n",
    "        # As pages complete, gather the results\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                page_books = future.result()\n",
    "                all_books.extend(page_books)\n",
    "            except Exception as exc:\n",
    "                print(f\"Error processing a future: {exc}\")\n",
    "\n",
    "    # Convert the list of dictionaries to a pandas DataFrame\n",
    "    df = pd.DataFrame(all_books)\n",
    "\n",
    "    # Remove duplicates based on book_id and url columns\n",
    "    df.drop_duplicates(subset=['book_id', 'url'], inplace=True)\n",
    "\n",
    "    # Save the cleaned DataFrame to a CSV file\n",
    "    df.to_csv('books_with_ids3.csv', index=False)\n",
    "    \n",
    "    print(f\"Total unique books fetched: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "# Run the function to fetch all pages\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_all_pages(total_pages=89)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
