{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e06ecd1e-98f0-485c-8f0d-7facf57f9eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57234d50-e2b1-4026-bac1-d3e7a143e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers    = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "# url        = 'https://www.goodreads.com/review/list/73221135?page=1'\n",
    "# response   = requests.get(url, headers=headers)\n",
    "# soup       = BeautifulSoup(response.content,'html.parser')\n",
    "# temph1     = soup.find_all('a',class_='bookTitle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01e73d93-f93f-434d-8a40-e85a75943bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempa = soup.find_all('div',class_='value')\n",
    "# tempa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9af0c001-ad05-4786-bd0a-377cab566254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique books fetched: 2057\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Set headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to fetch and parse a page\n",
    "def fetch_page(page_num):\n",
    "    url = f\"https://www.goodreads.com/review/list/73221135?page={page_num}\"\n",
    "\n",
    "    # Use a session to handle retries\n",
    "    session = requests.Session()\n",
    "    retry = Retry(connect=5, backoff_factor=0.5)  # Retry failed connections 5 times\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception for 4XX/5XX HTTP status codes\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching page {page_num}: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Parse the content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all divs with class 'value'\n",
    "    tempa = soup.find_all('div', class_='value')\n",
    "\n",
    "    # Extract book_id and URL\n",
    "    books = []\n",
    "    for div in tempa:\n",
    "        a_tag = div.find('a', href=True)\n",
    "        if a_tag:\n",
    "            book_url = a_tag['href']\n",
    "            if '/book/show/' in book_url:\n",
    "                # Extract book_id before '-' and '.'\n",
    "                book_id = book_url.split('/book/show/')[1].split('-')[0]\n",
    "                book_id = book_id.split('.')[0]\n",
    "                books.append({'book_id': book_id, 'url': f\"https://www.goodreads.com{book_url}\"})\n",
    "    \n",
    "    return books\n",
    "\n",
    "# Main function to fetch multiple pages concurrently\n",
    "def fetch_all_pages(total_pages=103):\n",
    "    all_books = []\n",
    "\n",
    "    # Use ThreadPoolExecutor to fetch multiple pages concurrently\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(fetch_page, i) for i in range(1, total_pages+1)]\n",
    "\n",
    "        # As pages complete, gather the results\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                page_books = future.result()\n",
    "                all_books.extend(page_books)\n",
    "            except Exception as exc:\n",
    "                print(f\"Error processing a future: {exc}\")\n",
    "\n",
    "    # Convert the list of dictionaries to a pandas DataFrame\n",
    "    df = pd.DataFrame(all_books)\n",
    "\n",
    "    # Remove duplicates based on book_id and url columns\n",
    "    df.drop_duplicates(subset=['book_id', 'url'], inplace=True)\n",
    "\n",
    "    # Save the cleaned DataFrame to a CSV file\n",
    "    df.to_csv('books_with_ids2.csv', index=False)\n",
    "    \n",
    "    print(f\"Total unique books fetched: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "# Run the function to fetch all pages\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_all_pages(total_pages=103)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb635b7-6381-4d2d-9973-9fe5ea4714ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5bfdce-d277-4204-b588-642439ab3bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d440c040-0d20-457a-87d6-94e448049eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
