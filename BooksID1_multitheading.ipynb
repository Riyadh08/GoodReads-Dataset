{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5146ab9-c8d5-49b5-8cde-3c93683c7776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from time import sleep\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Define headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Set up retry logic for requests\n",
    "session = requests.Session()\n",
    "retry = Retry(\n",
    "    total=5,          # Retry up to 5 times\n",
    "    backoff_factor=1,  # Wait 1 second between retries, then exponentially increase\n",
    "    status_forcelist=[429, 500, 502, 503, 504],  # Retry on these HTTP status codes\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('https://', adapter)\n",
    "session.mount('http://', adapter)\n",
    "\n",
    "# Initialize the list to store book data\n",
    "master_list = []\n",
    "\n",
    "# Function to scrape a single page\n",
    "def scrape_page(i):\n",
    "    try:\n",
    "        url = f\"https://www.goodreads.com/shelf/show/bengali?page={i}\"\n",
    "        response = session.get(url, headers=headers, timeout=10)  # Set timeout to 10 seconds\n",
    "        response.raise_for_status()  # Raise HTTP errors\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        page_books = []\n",
    "\n",
    "        # Find all book items\n",
    "        for x in soup.find_all('a', {\"class\": \"bookTitle\"}):\n",
    "            # Extract the URL\n",
    "            book_url = 'https://www.goodreads.com' + x['href']\n",
    "\n",
    "            # Extract the book ID from the href URL\n",
    "            book_id = x['href'].split('/')[-1]  # Extracting the ID from the URL\n",
    "            book_id = book_id.split('.')[0]\n",
    "\n",
    "            # Append the data as a dictionary to the page_books list\n",
    "            page_books.append({\n",
    "                'Book_ID': book_id,\n",
    "                'URL': book_url\n",
    "            })\n",
    "\n",
    "        return page_books\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error occurred while scraping page {i}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Use ThreadPoolExecutor to scrape with 5 threads (adjust as needed)\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:  # Reduced from 10 to 5\n",
    "    futures = [executor.submit(scrape_page, i) for i in range(1, 25+1)]  # Adjust range as needed\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        master_list.extend(future.result())\n",
    "\n",
    "# Convert the master_list into a pandas DataFrame\n",
    "df = pd.DataFrame(master_list)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv('books_with_ids.csv', index=False)\n",
    "\n",
    "# Check the number of books scraped\n",
    "print(len(master_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3484ec0-9e6f-44c2-b7d6-609268d8799e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250\n"
     ]
    }
   ],
   "source": [
    "temp = []\n",
    "\n",
    "# Use the actual length of master_list to avoid index errors\n",
    "for i in range(len(master_list)):  \n",
    "    temp.append(master_list[i]['URL'])\n",
    "    # print(temp[i])\n",
    "\n",
    "print(len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "522b3a8b-5b54-4f3b-878a-9171d86ccfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in temp:  \n",
    "#     response = requests.get(x, headers=headers)\n",
    "#     soup     = BeautifulSoup(response.content, 'html.parser')\n",
    "#     tt = soup.find_all('div','ReviewerProfile__name')\n",
    "#     print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f543e1e-739b-4294-b675-c1022922344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# m_urls = []\n",
    "\n",
    "# for x in temp:\n",
    "#     try:\n",
    "#         response = requests.get(x, headers=headers, timeout=10)  # Setting a timeout\n",
    "#         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#         tt = soup.find_all('div', class_='ReviewerProfile__name')\n",
    "        \n",
    "#         # List to store the URLs\n",
    "#         urls = []\n",
    "        \n",
    "#         # Loop through each div and extract the href from the <a> tag\n",
    "#         for div in tt:\n",
    "#             a_tag = div.find('a')\n",
    "#             if a_tag and 'href' in a_tag.attrs:\n",
    "#                 urls.append(a_tag['href'])\n",
    "        \n",
    "#         # Append URLs to the master list\n",
    "#         m_urls.extend(urls)\n",
    "\n",
    "#         # Introduce a delay of 1 second between requests\n",
    "#         time.sleep(1)\n",
    "    \n",
    "#     except requests.exceptions.Timeout:\n",
    "#         print(f\"Request timed out for: {x}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb0a7424-4064-4551-b08b-102eec870cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Convert the urls into a pandas DataFrame\n",
    "    # df = pd.DataFrame(m_urls)\n",
    "    # df.to_csv('User_Url1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7efa60-b4ac-4398-8151-22a217c3b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(m_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46d8d091-fa5e-4984-a1c8-1b670bf47018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URLs extracted: 37175\n"
     ]
    }
   ],
   "source": [
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import time\n",
    "\n",
    "# Create a session object\n",
    "session = requests.Session()\n",
    "\n",
    "# Configure retries\n",
    "retries = Retry(total=5,  # Retry up to 5 times\n",
    "                backoff_factor=1,  # Wait 1 second between retries\n",
    "                status_forcelist=[500, 502, 503, 504],  # Retry on specific status codes\n",
    "                raise_on_status=False)\n",
    "\n",
    "# Mount the retry adapter to the session\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "m_urls = []  # List to store all URLs\n",
    "\n",
    "for x in temp:\n",
    "    try:\n",
    "        # Use the session to make requests with retry logic\n",
    "        response = session.get(x, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        tt = soup.find_all('div', class_='ReviewerProfile__name')\n",
    "        \n",
    "        # List to store the URLs for this request\n",
    "        urls = []\n",
    "        \n",
    "        # Loop through each div and extract the href from the <a> tag\n",
    "        for div in tt:\n",
    "            a_tag = div.find('a')\n",
    "            if a_tag and 'href' in a_tag.attrs:\n",
    "                urls.append(a_tag['href'])\n",
    "        \n",
    "        # Append URLs to the master list\n",
    "        m_urls.extend(urls)\n",
    "\n",
    "        # Introduce a delay of 1 second between requests to avoid server overloading\n",
    "        time.sleep(1)\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed for: {x}, error: {e}\")\n",
    "\n",
    "# After the loop, convert the URLs into a pandas DataFrame and save to a CSV\n",
    "df = pd.DataFrame(m_urls)\n",
    "df.to_csv('User_Url.csv', index=False)\n",
    "\n",
    "print(f\"Total URLs extracted: {len(m_urls)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da44e501-99d3-47bd-8abd-670ce3ed470c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0273f59-4c7d-432a-b171-068a0c4c6219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
