{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5146ab9-c8d5-49b5-8cde-3c93683c7776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from time import sleep\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Define headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Set up retry logic for requests\n",
    "session = requests.Session()\n",
    "retry = Retry(\n",
    "    total=5,          # Retry up to 5 times\n",
    "    backoff_factor=1,  # Wait 1 second between retries, then exponentially increase\n",
    "    status_forcelist=[429, 500, 502, 503, 504],  # Retry on these HTTP status codes\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('https://', adapter)\n",
    "session.mount('http://', adapter)\n",
    "\n",
    "# Initialize the list to store book data\n",
    "master_list = []\n",
    "\n",
    "# Function to scrape a single page\n",
    "def scrape_page(i):\n",
    "    try:\n",
    "        url = f\"https://www.goodreads.com/shelf/show/bengali?page={i}\"\n",
    "        response = session.get(url, headers=headers, timeout=10)  # Set timeout to 10 seconds\n",
    "        response.raise_for_status()  # Raise HTTP errors\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        page_books = []\n",
    "\n",
    "        # Find all book items\n",
    "        for x in soup.find_all('a', {\"class\": \"bookTitle\"}):\n",
    "            # Extract the URL\n",
    "            book_url = 'https://www.goodreads.com' + x['href']\n",
    "\n",
    "            # Extract the book ID from the href URL\n",
    "            book_id = x['href'].split('/')[-1]  # Extracting the ID from the URL\n",
    "            book_id = book_id.split('.')[0]\n",
    "\n",
    "            # Append the data as a dictionary to the page_books list\n",
    "            page_books.append({\n",
    "                'Book_ID': book_id,\n",
    "                'URL': book_url\n",
    "            })\n",
    "\n",
    "        return page_books\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error occurred while scraping page {i}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Use ThreadPoolExecutor to scrape with 5 threads (adjust as needed)\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:  # Reduced from 10 to 5\n",
    "    futures = [executor.submit(scrape_page, i) for i in range(1, 25+1)]  # Adjust range as needed\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        master_list.extend(future.result())\n",
    "\n",
    "# Convert the master_list into a pandas DataFrame\n",
    "df = pd.DataFrame(master_list)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv('books_with_ids.csv', index=False)\n",
    "\n",
    "# Check the number of books scraped\n",
    "print(len(master_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3484ec0-9e6f-44c2-b7d6-609268d8799e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
