{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5146ab9-c8d5-49b5-8cde-3c93683c7776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from time import sleep\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Define headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Set up retry logic for requests\n",
    "session = requests.Session()\n",
    "retry = Retry(\n",
    "    total=5,          # Retry up to 5 times\n",
    "    backoff_factor=1,  # Wait 1 second between retries, then exponentially increase\n",
    "    status_forcelist=[429, 500, 502, 503, 504],  # Retry on these HTTP status codes\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('https://', adapter)\n",
    "session.mount('http://', adapter)\n",
    "\n",
    "# Initialize the list to store book data\n",
    "master_list = []\n",
    "\n",
    "# Function to scrape a single page\n",
    "def scrape_page(i):\n",
    "    try:\n",
    "        url = f\"https://www.goodreads.com/shelf/show/bengali?page={i}\"\n",
    "        response = session.get(url, headers=headers, timeout=10)  # Set timeout to 10 seconds\n",
    "        response.raise_for_status()  # Raise HTTP errors\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        page_books = []\n",
    "\n",
    "        # Find all book items\n",
    "        for x in soup.find_all('a', {\"class\": \"bookTitle\"}):\n",
    "            # Extract the URL\n",
    "            book_url = 'https://www.goodreads.com' + x['href']\n",
    "\n",
    "            # Extract the book ID from the href URL\n",
    "            book_id = x['href'].split('/')[-1]  # Extracting the ID from the URL\n",
    "            book_id = book_id.split('.')[0]\n",
    "\n",
    "            # Append the data as a dictionary to the page_books list\n",
    "            page_books.append({\n",
    "                'Book_ID': book_id,\n",
    "                'URL': book_url\n",
    "            })\n",
    "\n",
    "        return page_books\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error occurred while scraping page {i}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Use ThreadPoolExecutor to scrape with 5 threads (adjust as needed)\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:  # Reduced from 10 to 5\n",
    "    futures = [executor.submit(scrape_page, i) for i in range(1, 25+1)]  # Adjust range as needed\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        master_list.extend(future.result())\n",
    "\n",
    "# Convert the master_list into a pandas DataFrame\n",
    "df = pd.DataFrame(master_list)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv('books_with_ids.csv', index=False)\n",
    "\n",
    "# Check the number of books scraped\n",
    "print(len(master_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3484ec0-9e6f-44c2-b7d6-609268d8799e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250\n"
     ]
    }
   ],
   "source": [
    "temp = []\n",
    "\n",
    "# Use the actual length of master_list to avoid index errors\n",
    "for i in range(len(master_list)):  \n",
    "    temp.append(master_list[i]['URL'])\n",
    "    # print(temp[i])\n",
    "\n",
    "print(len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "522b3a8b-5b54-4f3b-878a-9171d86ccfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in temp:  \n",
    "#     response = requests.get(x, headers=headers)\n",
    "#     soup     = BeautifulSoup(response.content, 'html.parser')\n",
    "#     tt = soup.find_all('div','ReviewerProfile__name')\n",
    "#     print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f543e1e-739b-4294-b675-c1022922344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# m_urls = []\n",
    "\n",
    "# for x in temp:\n",
    "#     try:\n",
    "#         response = requests.get(x, headers=headers, timeout=10)  # Setting a timeout\n",
    "#         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#         tt = soup.find_all('div', class_='ReviewerProfile__name')\n",
    "        \n",
    "#         # List to store the URLs\n",
    "#         urls = []\n",
    "        \n",
    "#         # Loop through each div and extract the href from the <a> tag\n",
    "#         for div in tt:\n",
    "#             a_tag = div.find('a')\n",
    "#             if a_tag and 'href' in a_tag.attrs:\n",
    "#                 urls.append(a_tag['href'])\n",
    "        \n",
    "#         # Append URLs to the master list\n",
    "#         m_urls.extend(urls)\n",
    "\n",
    "#         # Introduce a delay of 1 second between requests\n",
    "#         time.sleep(1)\n",
    "    \n",
    "#     except requests.exceptions.Timeout:\n",
    "#         print(f\"Request timed out for: {x}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb0a7424-4064-4551-b08b-102eec870cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Convert the urls into a pandas DataFrame\n",
    "    # df = pd.DataFrame(m_urls)\n",
    "    # df.to_csv('User_Url1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7efa60-b4ac-4398-8151-22a217c3b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(m_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46d8d091-fa5e-4984-a1c8-1b670bf47018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URLs extracted: 37175\n"
     ]
    }
   ],
   "source": [
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import time\n",
    "\n",
    "# Create a session object\n",
    "session = requests.Session()\n",
    "\n",
    "# Configure retries\n",
    "retries = Retry(total=5,  # Retry up to 5 times\n",
    "                backoff_factor=1,  # Wait 1 second between retries\n",
    "                status_forcelist=[500, 502, 503, 504],  # Retry on specific status codes\n",
    "                raise_on_status=False)\n",
    "\n",
    "# Mount the retry adapter to the session\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "m_urls = []  # List to store all URLs\n",
    "\n",
    "for x in temp:\n",
    "    try:\n",
    "        # Use the session to make requests with retry logic\n",
    "        response = session.get(x, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        tt = soup.find_all('div', class_='ReviewerProfile__name')\n",
    "        \n",
    "        # List to store the URLs for this request\n",
    "        urls = []\n",
    "        \n",
    "        # Loop through each div and extract the href from the <a> tag\n",
    "        for div in tt:\n",
    "            a_tag = div.find('a')\n",
    "            if a_tag and 'href' in a_tag.attrs:\n",
    "                urls.append(a_tag['href'])\n",
    "        \n",
    "        # Append URLs to the master list\n",
    "        m_urls.extend(urls)\n",
    "\n",
    "        # Introduce a delay of 1 second between requests to avoid server overloading\n",
    "        time.sleep(1)\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed for: {x}, error: {e}\")\n",
    "\n",
    "# After the loop, convert the URLs into a pandas DataFrame and save to a CSV\n",
    "df = pd.DataFrame(m_urls)\n",
    "df.to_csv('User_Url.csv', index=False)\n",
    "\n",
    "print(f\"Total URLs extracted: {len(m_urls)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86325696-ae05-4969-a3b4-48191ece3036",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 39\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m temp:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m# Skip if the URL was already visited\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         \n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m         \u001b[38;5;66;03m# Use the session to make requests with retry logic\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m         response \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mget(x, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     40\u001b[0m         soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     41\u001b[0m         tt \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReviewerProfile__name\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\requests\\sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 747\u001b[0m     r\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\requests\\models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\urllib3\\response.py:933\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m--> 933\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\urllib3\\response.py:1073\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1073\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_chunk_length()\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1075\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\urllib3\\response.py:1001\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1000\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1001\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\ssl.py:1315\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1313\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1314\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Read the `temp` list from the `books_with_ids.csv` file\n",
    "books_with_ids = pd.read_csv('books_with_ids.csv')\n",
    "temp = books_with_ids['URL'].tolist()  # Assuming the file has a column named 'urls'\n",
    "\n",
    "# Create a session object\n",
    "session = requests.Session()\n",
    "\n",
    "# Configure retries\n",
    "retries = Retry(total=5,  # Retry up to 5 times\n",
    "                backoff_factor=1,  # Wait 1 second between retries\n",
    "                status_forcelist=[500, 502, 503, 504],  # Retry on specific status codes\n",
    "                raise_on_status=False)\n",
    "\n",
    "# Mount the retry adapter to the session\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "m_urls = []  # List to store all URLs\n",
    "visited = set()  # Set to track visited URLs\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "for x in temp:\n",
    "    try:\n",
    "        # Skip if the URL was already visited\n",
    "        \n",
    "\n",
    "        # Use the session to make requests with retry logic\n",
    "        response = session.get(x, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        tt = soup.find_all('div', class_='ReviewerProfile__name')\n",
    "        \n",
    "        # Loop through each div and extract the href from the <a> tag\n",
    "        for div in tt:\n",
    "            a_tag = div.find('a')\n",
    "            if a_tag and 'href' in a_tag.attrs:\n",
    "                href = a_tag['href']\n",
    "                if href not in visited:  # Check if the href is already visited\n",
    "                    m_urls.append(href)\n",
    "                    visited.add(href)  # Mark this href as visited\n",
    "\n",
    "       # print(f\"Processed {x}: Extracted {len(m_urls)} total URLs so far.\")\n",
    "\n",
    "        # Mark the current URL as visited\n",
    "        #visited.add(x)\n",
    "\n",
    "        # Introduce a delay of 1 second between requests to avoid server overloading\n",
    "        time.sleep(1)\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed for: {x}, error: {e}\")\n",
    "\n",
    "# Save the extracted URLs to a new CSV file called 'bengali_users.csv'\n",
    "df = pd.DataFrame(m_urls, columns=['URLs'])\n",
    "df.to_csv('bengali_users.csv', index=False)\n",
    "\n",
    "print(f\"Total URLs extracted: {len(m_urls)}\")\n",
    "print(\"Scraping completed. Data saved to 'bengali_users.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da44e501-99d3-47bd-8abd-670ce3ed470c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0273f59-4c7d-432a-b171-068a0c4c6219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
