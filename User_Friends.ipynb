{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "892933e9-4150-43ca-ab74-f22943e4d961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=1&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 1\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=2&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 2\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=3&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 3\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=4&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 4\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=5&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 5\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=6&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 6\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=7&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 7\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=8&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 8\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=9&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 9\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=10&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 10\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=11&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 11\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=12&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 12\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=13&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 13\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=14&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 14\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=15&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 15\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=16&skip_mutual_friends=false\n",
      "Error scraping https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=16&skip_mutual_friends=false: Page.goto: Timeout 30000ms exceeded.\n",
      "Call log:\n",
      "navigating to \"https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=16&skip_mutual_friends=false\", waiting until \"load\"\n",
      "\n",
      "Scraped 0 friends from page 16\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=17&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 17\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=18&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 18\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=19&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 19\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=20&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 20\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=21&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 21\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=22&skip_mutual_friends=false\n",
      "Error scraping https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=22&skip_mutual_friends=false: Page.goto: Timeout 30000ms exceeded.\n",
      "Call log:\n",
      "navigating to \"https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=22&skip_mutual_friends=false\", waiting until \"load\"\n",
      "\n",
      "Scraped 0 friends from page 22\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=23&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 23\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=24&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 24\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=25&skip_mutual_friends=false\n",
      "Error scraping https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=25&skip_mutual_friends=false: Page.goto: Timeout 30000ms exceeded.\n",
      "Call log:\n",
      "navigating to \"https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=25&skip_mutual_friends=false\", waiting until \"load\"\n",
      "\n",
      "Scraped 0 friends from page 25\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=26&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 26\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=27&skip_mutual_friends=false\n",
      "Error scraping https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=27&skip_mutual_friends=false: Page.goto: Timeout 30000ms exceeded.\n",
      "Call log:\n",
      "navigating to \"https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=27&skip_mutual_friends=false\", waiting until \"load\"\n",
      "\n",
      "Scraped 0 friends from page 27\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=28&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 28\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=29&skip_mutual_friends=false\n",
      "Scraped 30 friends from page 29\n",
      "Scraping: https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=30&skip_mutual_friends=false\n",
      "Scraped 29 friends from page 30\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "import csv\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Apply the nested asyncio patch\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def scrape_friend_urls(url, page):\n",
    "    try:\n",
    "        # Navigate to the user's friend list page\n",
    "        await page.goto(url)\n",
    "\n",
    "        # Wait for the friend table to load\n",
    "        await page.wait_for_selector(\"#friendTable\", timeout=10000)\n",
    "\n",
    "        # Extract the page content\n",
    "        content = await page.content()\n",
    "\n",
    "        # Parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        # Find all friend profile links\n",
    "        friends = soup.select('table#friendTable td[width=\"30%\"] a[rel=\"acquaintance\"]')\n",
    "        friend_urls = [\"https://www.goodreads.com\" + friend['href'] for friend in friends]\n",
    "\n",
    "        return friend_urls\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "async def main():\n",
    "    # Define the base URL for the friend list\n",
    "    base_url = \"https://www.goodreads.com/friend/user/26278904-monika-ghosh?page=\"\n",
    "    total_pages = 30  # Adjust based on the total number of pages in the friend list\n",
    "\n",
    "    # Prepare CSV file to save the friend list\n",
    "    with open('friend_urls.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Friend URL'])  # Write header\n",
    "\n",
    "        async with async_playwright() as p:\n",
    "            # Launch a browser\n",
    "            browser = await p.chromium.launch(headless=False)  # Use headless=True for headless mode\n",
    "            context = await browser.new_context()\n",
    "            page = await context.new_page()\n",
    "\n",
    "            # Log in to Goodreads\n",
    "            await page.goto(\"https://www.goodreads.com/ap/signin?language=en_US&openid.assoc_handle=amzn_goodreads_web_na&openid.claimed_id=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select&openid.identity=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select&openid.mode=checkid_setup&openid.ns=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0&openid.pape.max_auth_age=0&openid.return_to=https%3A%2F%2Fwww.goodreads.com%2Fap-handler%2Fsign-in&siteState=eyJyZXR1cm5fdXJsIjoiaHR0cHM6Ly93d3cuZ29vZHJlYWRzLmNvbS8ifQ%3D%3D\")\n",
    "            await page.wait_for_selector(\"input[name='email']\", timeout=5000)\n",
    "            await page.fill(\"input[name='email']\", \"robiulriyadh66@gmail.com\")  # Your email\n",
    "            await page.fill(\"input[name='password']\", \"V!MZUhP8TL39-jH\")        # Your password\n",
    "            await page.click(\"input[type='submit']\")\n",
    "            await page.wait_for_selector(\".siteHeader__topLevelLink\", timeout=30000)\n",
    "\n",
    "            # Iterate over all pages of the friend list\n",
    "            for page_number in range(1, total_pages + 1):\n",
    "                current_url = f\"{base_url}{page_number}&skip_mutual_friends=false\"\n",
    "                print(f\"Scraping: {current_url}\")\n",
    "\n",
    "                # Scrape friend URLs from the current page\n",
    "                friend_urls = await scrape_friend_urls(current_url, page)\n",
    "\n",
    "                # Write the URLs to the CSV file\n",
    "                for friend_url in friend_urls:\n",
    "                    writer.writerow([friend_url])\n",
    "                print(f\"Scraped {len(friend_urls)} friends from page {page_number}\")\n",
    "\n",
    "            # Close the browser\n",
    "            await browser.close()\n",
    "\n",
    "# Run the async function\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962320d6-8807-4998-a5de-a0bfe4bc2321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6807348-f19f-4248-a661-f642cb6d710c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45108893-38cc-4bd5-ac2d-35e2a2e40eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a6b73b-ef57-428f-b2e7-1b7f37364b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c334b-d102-4a1d-8d76-1beaa919d967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
